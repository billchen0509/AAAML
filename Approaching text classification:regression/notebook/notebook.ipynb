{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(sentence, pos, neg):\n",
    "    '''\n",
    "    This function returns sentiment of sentence\n",
    "    :param sentence: str\n",
    "    :param pos: list\n",
    "    :param neg: list\n",
    "    :return: returns positive, negative or neutral\n",
    "    '''\n",
    "\n",
    "    # Split sentence into words\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    # make sentence into a set\n",
    "    sentence = set(sentence)\n",
    "\n",
    "    # check number of common words with positive\n",
    "    num_common_pos = len(sentence.intersection(pos))\n",
    "\n",
    "    # check number of common words with negative\n",
    "    num_common_neg = len(sentence.intersection(neg))\n",
    "\n",
    "    # make conditions and return\n",
    "    if num_common_pos > num_common_neg:\n",
    "        return 'positive'\n",
    "    elif num_common_pos < num_common_neg:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a corpus of sentences\n",
    "corpus = [\n",
    "    'hello, how are you?',\n",
    "    'im getting bored at home. And you? What do you think?',\n",
    "    'did you know about counts',\n",
    "    \"let's see if this works!\",\n",
    "    'YES!!'\n",
    "]\n",
    "\n",
    "# initialize CountVectorizer\n",
    "ctv = CountVectorizer()\n",
    "\n",
    "# fit the vectorizer on corpus\n",
    "ctv.fit(corpus)\n",
    "\n",
    "corpus_transformed = ctv.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 22)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 22)\t2\n",
      "  (2, 0)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 22)\t1\n",
      "  (3, 12)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 20)\t1\n",
      "  (4, 21)\t1\n"
     ]
    }
   ],
   "source": [
    "print(corpus_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/billhikari/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 14, ',': 2, 'how': 16, 'are': 7, 'you': 27, '?': 4, 'im': 18, 'getting': 13, 'bored': 9, 'at': 8, 'home': 15, '.': 3, 'and': 6, 'what': 24, 'do': 12, 'think': 22, 'did': 11, 'know': 19, 'about': 5, 'counts': 10, 'let': 20, \"'s\": 1, 'see': 21, 'if': 17, 'this': 23, 'works': 25, '!': 0, 'yes': 26}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# create a corpus of sentences\n",
    "corpus = [\n",
    "    'hello, how are you?',\n",
    "    'im getting bored at home. And you? What do you think?',\n",
    "    'did you know about counts',\n",
    "    \"let's see if this works!\",\n",
    "    'YES!!'\n",
    "]\n",
    "\n",
    "# initialize CountVectorizer with word_tokenize from nltk\n",
    "ctv = CountVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "\n",
    "# fit the vectorizer on corpus\n",
    "ctv.fit(corpus)\n",
    "\n",
    "corpus_transformed = ctv.transform(corpus)\n",
    "print(ctv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 27)\t0.2965698850220162\n",
      "  (0, 16)\t0.4428321995085722\n",
      "  (0, 14)\t0.4428321995085722\n",
      "  (0, 7)\t0.4428321995085722\n",
      "  (0, 4)\t0.35727423026525224\n",
      "  (0, 2)\t0.4428321995085722\n",
      "  (1, 27)\t0.35299699146792735\n",
      "  (1, 24)\t0.2635440111190765\n",
      "  (1, 22)\t0.2635440111190765\n",
      "  (1, 18)\t0.2635440111190765\n",
      "  (1, 15)\t0.2635440111190765\n",
      "  (1, 13)\t0.2635440111190765\n",
      "  (1, 12)\t0.2635440111190765\n",
      "  (1, 9)\t0.2635440111190765\n",
      "  (1, 8)\t0.2635440111190765\n",
      "  (1, 6)\t0.2635440111190765\n",
      "  (1, 4)\t0.42525129752567803\n",
      "  (1, 3)\t0.2635440111190765\n",
      "  (2, 27)\t0.31752680284846835\n",
      "  (2, 19)\t0.4741246485558491\n",
      "  (2, 11)\t0.4741246485558491\n",
      "  (2, 10)\t0.4741246485558491\n",
      "  (2, 5)\t0.4741246485558491\n",
      "  (3, 25)\t0.38775666010579296\n",
      "  (3, 23)\t0.38775666010579296\n",
      "  (3, 21)\t0.38775666010579296\n",
      "  (3, 20)\t0.38775666010579296\n",
      "  (3, 17)\t0.38775666010579296\n",
      "  (3, 1)\t0.38775666010579296\n",
      "  (3, 0)\t0.3128396318588854\n",
      "  (4, 26)\t0.52677824987419\n",
      "  (4, 0)\t0.8500027502658362\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create a corpus of sentences\n",
    "corpus = [\n",
    "    'hello, how are you?',\n",
    "    'im getting bored at home. And you? What do you think?',\n",
    "    'did you know about counts',\n",
    "    \"let's see if this works!\",\n",
    "    'YES!!'\n",
    "]\n",
    "\n",
    "# initialize TfidfVectorizer with word_tokenize from nltk\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "\n",
    "# fit the vectorizer on corpus\n",
    "tfv.fit(corpus)\n",
    "\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "print(corpus_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', ',', 'how'), (',', 'how', 'are'), ('how', 'are', 'you'), ('are', 'you', '?')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "N = 3\n",
    "\n",
    "sentence = 'hello, how are you?'\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "n_grams = list(ngrams(tokenized_sentence, N))\n",
    "print(n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/billhikari/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word = fishing\n",
      "lemma = fishing\n",
      "stem = fish\n",
      "word = fishes\n",
      "lemma = fish\n",
      "stem = fish\n",
      "word = fished\n",
      "lemma = fished\n",
      "stem = fish\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "words = ['fishing', 'fishes', 'fished']\n",
    "\n",
    "for word in words:\n",
    "    print(f'word = {word}')\n",
    "    print(f'lemma = {lemmatizer.lemmatize(word)}')\n",
    "    print(f'stem = {stemmer.stem(word)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'a', 'and']\n",
      "['br', '<', '>', '/', '-']\n",
      "['i', 'movie', '!', 'it', 'was']\n",
      "[',', '!', \"''\", '``', 'you']\n",
      "['!', 'the', \"''\", '``', '...']\n"
     ]
    }
   ],
   "source": [
    "# topic extraction\n",
    "# non-negative matrix fractionization (NMF)\n",
    "# latent semantic analysis (LSA), which is also known as singular value decomposition (SVD)\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a corpus of sentences\n",
    "corpus = pd.read_csv('../input/imdb.csv', nrows=10000)\n",
    "corpus = corpus.review.values  # corpus.review.values is a numpy array. type: numpy.ndarray\n",
    "# corpus = corpus.review # corpus.review is a pandas series. type: pandas.Series\n",
    "\n",
    "# initialize TfidfVectorizer\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "\n",
    "# fit the vectorizer on corpus\n",
    "tfv.fit(corpus)\n",
    "\n",
    "# transform the corpus\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "\n",
    "# initialize SVD with 10 components\n",
    "svd = decomposition.TruncatedSVD(n_components=10)\n",
    "\n",
    "# fit SVD\n",
    "corpus_svd = svd.fit(corpus_transformed)\n",
    "\n",
    "# choose first sample and create a dictionary of feature names and their scores from \n",
    "# svd components.\n",
    "sample_index = 0\n",
    "feature_scores = dict(\n",
    "    zip(\n",
    "        tfv.get_feature_names_out(),\n",
    "        corpus_svd.components_[sample_index]\n",
    "    )\n",
    ")\n",
    "\n",
    "# sort the feature names based on their scores\n",
    "N = 5\n",
    "\n",
    "for sample_index in range(5):\n",
    "    feature_scores = dict(\n",
    "        zip(\n",
    "            tfv.get_feature_names_out(),\n",
    "            corpus_svd.components_[sample_index]\n",
    "        )\n",
    "    )\n",
    "    print(sorted(feature_scores, key=feature_scores.get, reverse=True)[:N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use corpus.review (pandas Series) when:\n",
    "\n",
    "You need to perform pandas-specific operations, like data manipulation, cleaning, or aggregation.\n",
    "\n",
    "You need to leverage the rich set of methods available in pandas for text processing, filtering, and more.\n",
    "\n",
    "- Use corpus.review.values (NumPy array) when:\n",
    "\n",
    "You need to pass the data to functions or libraries that require NumPy arrays.\n",
    "\n",
    "You want to perform operations that are more efficient or simpler with NumPy arrays, such as vectorized operations or interfacing with machine learning libraries like scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Purpose of transform\n",
    "\n",
    "The transform method is crucial because it converts the text data into numerical form. Machine learning models require numerical input, and the TF-IDF representation is a commonly used method for converting text data into numerical features. Here’s why this step is important:\n",
    "\n",
    "- Numerical Representation:\n",
    "\n",
    "Text data cannot be directly used by machine learning algorithms that require numerical input.\n",
    "\n",
    "The transform method converts the corpus into a sparse matrix of TF-IDF features, which can be used by these algorithms.\n",
    "\n",
    "- Feature Extraction:\n",
    "\n",
    "The TF-IDF (Term Frequency-Inverse Document Frequency) method provides a measure of how important a word is to a document in a corpus.\n",
    "\n",
    "This transformation helps to highlight the important words in the corpus while reducing the impact of less important ones.\n",
    "\n",
    "- Consistency:\n",
    "\n",
    "The fit method builds the vocabulary and computes IDF values based on the corpus.\n",
    "\n",
    "The transform method uses this learned information to ensure that the same vocabulary and weighting are applied consistently across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to clean any text data, we can use the following function\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(s):\n",
    "    '''\n",
    "    This function cleans the text data\n",
    "    :param s: str\n",
    "    :return: str\n",
    "    '''\n",
    "\n",
    "    # split by all whitespaces\n",
    "    s= s.split()\n",
    "\n",
    "    # join tokens by singe  space\n",
    "    # this will remove all kinds of spaces including tab, newline, etc.\n",
    "    # and replaces it with a single space\n",
    "    s = ' '.join(s)\n",
    "\n",
    "    # remove all punctuations using regex and string module\n",
    "    s = re.sub(f'[{re.escape(string.punctuation)}]', '', s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'a', 'and', 'of', 'to']\n",
      "['i', 'movie', 'it', 'was', 'this']\n",
      "['the', 'was', 'i', 'were', 'of']\n",
      "['her', 'was', 'she', 'i', 'he']\n",
      "['br', 'to', 'they', 'he', 'show']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a corpus of sentences\n",
    "corpus = pd.read_csv('../input/imdb.csv', nrows=10000)\n",
    "corpus.loc[:, 'review'] = corpus.review.apply(clean_text)\n",
    "\n",
    "corpus = corpus.review.values\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "tfv.fit(corpus)\n",
    "\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "\n",
    "svd = decomposition.TruncatedSVD(n_components=10)\n",
    "corpus_svd = svd.fit(corpus_transformed)\n",
    "\n",
    "sample_index = 0\n",
    "feature_scores = dict(\n",
    "    zip(\n",
    "        tfv.get_feature_names_out(),\n",
    "        corpus_svd.components_[sample_index]\n",
    "    )\n",
    ")\n",
    "\n",
    "N = 5\n",
    "\n",
    "for sample_index in range(5):\n",
    "    feature_scores = dict(\n",
    "        zip(\n",
    "            tfv.get_feature_names_out(),\n",
    "            corpus_svd.components_[sample_index]\n",
    "        )\n",
    "    )\n",
    "    print(sorted(feature_scores, key=feature_scores.get, reverse=True)[:N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove stopwords in cleaning function, but is not always a wisw choice and depends a lot on the business problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take all individual word vectors in a given sentence and create a normalized word vector from all word vectors of the tokens. \n",
    "\n",
    "This provdie us with a sentence vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sentence_to_vec(s, embedding_dict, stop_words, tokenizer):\n",
    "    '''Given a sentence and other information, this function returns embedding for the whole sentence\n",
    "    :param s: sentence, string\n",
    "    :param embedding_dict: dictionary, a dictionary that has words as keys and their embeddings as values\n",
    "    :param stop_words: list of stop words, if any\n",
    "    :param tokenizer: a Tokenizer object\n",
    "    '''\n",
    "\n",
    "    # convert sentence to string and lowercase it\n",
    "    words = str(s).lower()\n",
    "\n",
    "    # tokenize the sentence\n",
    "    words = tokenizer(words)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    # keep only alpha-numeric tokens\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "\n",
    "    # initialize empty list to store embeddings\n",
    "\n",
    "    M = []\n",
    "\n",
    "    for w in words:\n",
    "        # for every word, fetch the embedding from the dictionary and append to list of embeddings\n",
    "        if w in embedding_dict:\n",
    "            M.append(embedding_dict[w])\n",
    "    if len(M) == 0:\n",
    "        return np.zeros(300)\n",
    "\n",
    "    # convert list of embeddings to array\n",
    "    M = np.array(M)\n",
    "\n",
    "    # calculate sum over axis 0\n",
    "    v = M.sum(axis=0)\n",
    "\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
